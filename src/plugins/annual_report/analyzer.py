"""
QQ ç¾¤å¹´åº¦çƒ­è¯åˆ†æå™¨

æœ¬æ¨¡å—åŸºäº https://github.com/ZiHuixi/QQgroup-annual-report-analyzer/commit/e0f0c474191c278da6be4857e99207a3127eec6e
åœ¨ MIT åè®®ä¸‹ä¿®æ”¹å’Œä½¿ç”¨

åŸé¡¹ç›®ç‰ˆæƒï¼šCopyright (c) 2025 ZiHuixi
"""

import math
import random
import re
import string
from collections import Counter, defaultdict
from typing import Any

import jieba
from nonebot import logger

from .config import config
from .schema import AnalyzerInput, Message
from .utils import (
    analyze_single_chars,
    calculate_entropy,
    clean_text,
    extract_emojis,
    is_emoji,
    parse_timestamp,
)

PUNCTUATION = string.punctuation + "ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€''ï¼ˆï¼‰ã€ã€‘"


class ChatAnalyzer:
    """QQ ç¾¤èŠåˆ†æå™¨"""

    def __init__(self, data: AnalyzerInput) -> None:
        """åˆå§‹åŒ–åˆ†æå™¨

        Args:
            data: ç¬¦åˆ AnalyzerInput æ¨¡å‹çš„è¾“å…¥æ•°æ®
        """
        self.data = data
        self.messages = data.messages
        self.chat_name: str = (
            data.chatName
            or (data.chatInfo.name if data.chatInfo else None)
            or "æœªçŸ¥ç¾¤èŠ"
        )

        # æ˜ å°„å’Œç»Ÿè®¡
        self.uin_to_name: dict[str | int, str] = {}
        self.msgid_to_sender: dict[str, str | int] = {}

        # è¯é¢‘ç»Ÿè®¡
        self.word_freq: Counter[str] = Counter()
        self.word_samples: defaultdict[str, list[str]] = defaultdict(list)
        self.word_contributors: defaultdict[str, Counter[str | int]] = defaultdict(
            Counter
        )

        # ç”¨æˆ·ç»Ÿè®¡
        self.user_msg_count: Counter[str | int] = Counter()
        self.user_char_count: Counter[str | int] = Counter()
        self.user_char_per_msg: dict[str | int, float] = {}
        self.user_image_count: Counter[str | int] = Counter()
        self.user_forward_count: Counter[str | int] = Counter()
        self.user_reply_count: Counter[str | int] = Counter()
        self.user_replied_count: Counter[str | int] = Counter()
        self.user_at_count: Counter[str | int] = Counter()
        self.user_ated_count: Counter[str | int] = Counter()
        self.user_emoji_count: Counter[str | int] = Counter()
        self.user_link_count: Counter[str | int] = Counter()
        self.user_night_count: Counter[str | int] = Counter()
        self.user_morning_count: Counter[str | int] = Counter()
        self.user_repeat_count: Counter[str | int] = Counter()

        # æ—¶é—´åˆ†å¸ƒ
        self.hour_distribution: Counter[int] = Counter()

        # æ–°è¯å‘ç°å’Œåˆå¹¶
        self.discovered_words: set[str] = set()
        self.merged_words: dict[str, tuple[str, str, int, float]] = {}

        # å•å­—ç»Ÿè®¡
        self.single_char_stats: dict[str, tuple[int, float, float]] = {}
        self.cleaned_texts: list[str] = []

        self._build_mappings()

    def _is_bot_message(self, msg: Message) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœºå™¨äººæ¶ˆæ¯ï¼ˆåŸºäº subMsgTypeï¼‰

        Args:
            msg: æ¶ˆæ¯å¯¹è±¡

        Returns:
            æ˜¯å¦ä¸ºæœºå™¨äººæ¶ˆæ¯
        """
        if not config.filter.filter_bot_messages:
            return False

        sub_msg_type: int = msg.rawMessage.subMsgType
        return sub_msg_type in [577, 65]

    def _build_mappings(self) -> None:
        """æ„å»º UIN åˆ°æ˜µç§°çš„æ˜ å°„ï¼Œä¼˜å…ˆä¿ç•™æœ‰æ•ˆçš„ name"""
        uin_names: defaultdict[str | int, list[str]] = defaultdict(list)
        uin_member_names: dict[str | int, str] = {}

        for msg in self.messages:
            if self._is_bot_message(msg):
                continue

            uin: str | int = msg.sender.uin
            name: str = msg.sender.name.strip()
            msg_id: str = msg.messageId

            # æ”¶é›† name
            if uin and name and (not uin_names[uin] or uin_names[uin][-1] != name):
                uin_names[uin].append(name)

            # æ”¶é›† sendMemberNameï¼ˆä¿ç•™æœ€åä¸€ä¸ªï¼‰
            send_member_name: str | None = msg.rawMessage.sendMemberName
            if uin and send_member_name:
                uin_member_names[uin] = send_member_name.strip()

            # æ„å»ºæ¶ˆæ¯ ID åˆ°å‘é€è€…çš„æ˜ å°„
            if msg_id and uin:
                self.msgid_to_sender[msg_id] = uin

        # ä¸ºæ¯ä¸ª UIN é€‰æ‹©æœ€åˆé€‚çš„ name
        for uin, names in uin_names.items():
            chosen_name: str | None = None

            # ä»åå¾€å‰æ‰¾ç¬¬ä¸€ä¸ªä¸ç­‰äº uin çš„ name
            for name in reversed(names):
                if name != str(uin):
                    chosen_name = name
                    break

            # å¦‚æœæ‰€æœ‰ name éƒ½ç­‰äº uinï¼Œä½¿ç”¨ sendMemberName
            if chosen_name is None:
                if uin in uin_member_names:
                    chosen_name = uin_member_names[uin]
                elif names:
                    chosen_name = names[-1]

            if chosen_name:
                self.uin_to_name[uin] = chosen_name

    def get_name(self, uin: str | int) -> str:
        """è·å–ç”¨æˆ·æ˜µç§°

        Args:
            uin: ç”¨æˆ· UIN

        Returns:
            ç”¨æˆ·æ˜µç§°
        """
        return self.uin_to_name.get(uin, f"æœªçŸ¥ç”¨æˆ·({uin})")

    def analyze(self) -> None:
        """æ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        logger.info(f"ğŸ“Š å¼€å§‹åˆ†æ: {self.chat_name}")
        logger.info(f"ğŸ“ æ¶ˆæ¯æ•°: {len(self.messages)}")

        logger.info("\nğŸ§¹ é¢„å¤„ç†æ–‡æœ¬...")
        self._preprocess_texts()

        logger.info("ğŸ”¤ åˆ†æå•å­—ç‹¬ç«‹æ€§...")
        self.single_char_stats = analyze_single_chars(self.cleaned_texts)

        logger.info("ğŸ” æ–°è¯å‘ç°...")
        self._discover_new_words()

        logger.info("ğŸ”— è¯ç»„åˆå¹¶...")
        self._merge_word_pairs()

        logger.info("ğŸ“ˆ åˆ†è¯ç»Ÿè®¡...")
        self._tokenize_and_count()

        logger.info("ğŸ® è¶£å‘³ç»Ÿè®¡...")
        self._fun_statistics()

        logger.info("ğŸ§¹ è¿‡æ»¤æ•´ç†...")
        self._filter_results()

        logger.info("âœ… å®Œæˆ!")

    def _preprocess_texts(self) -> None:
        """é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬"""
        skipped: int = 0
        bot_filtered: int = 0

        for msg in self.messages:
            if self._is_bot_message(msg):
                bot_filtered += 1
                continue

            text: str = msg.content.text
            cleaned: str = clean_text(text)

            if cleaned and len(cleaned) >= 1:
                self.cleaned_texts.append(cleaned)
            elif text:
                skipped += 1

        if config.filter.filter_bot_messages and bot_filtered > 0:
            logger.info(
                f"   æœ‰æ•ˆæ–‡æœ¬: {len(self.cleaned_texts)} æ¡, "
                f"è·³è¿‡: {skipped} æ¡, è¿‡æ»¤æœºå™¨äºº: {bot_filtered} æ¡"
            )
        else:
            logger.info(
                f"   æœ‰æ•ˆæ–‡æœ¬: {len(self.cleaned_texts)} æ¡, è·³è¿‡: {skipped} æ¡"
            )

    def _discover_new_words(self) -> None:
        """æ–°è¯å‘ç°"""
        ngram_freq: Counter[str] = Counter()
        left_neighbors: defaultdict[str, Counter[str]] = defaultdict(Counter)
        right_neighbors: defaultdict[str, Counter[str]] = defaultdict(Counter)
        total_chars: int = 0

        for text in self.cleaned_texts:
            sentences: list[str] = re.split(
                '[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""ï¼ˆï¼‰\\s\\n\\r,.!?()\\[\\]]', text
            )

            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) < 2:
                    continue

                total_chars += len(sentence)

                for n in range(2, min(6, len(sentence) + 1)):
                    for i in range(len(sentence) - n + 1):
                        ngram: str = sentence[i : i + n]

                        # è·³è¿‡çº¯æ•°å­—/ç¬¦å·/çº¯è‹±æ–‡
                        if re.match(r"^[\d\s\W]+$", ngram) or re.match(
                            r"^[a-zA-Z]+$", ngram
                        ):
                            continue

                        ngram_freq[ngram] += 1

                        if i > 0:
                            left_neighbors[ngram][sentence[i - 1]] += 1
                        else:
                            left_neighbors[ngram]["<BOS>"] += 1

                        if i + n < len(sentence):
                            right_neighbors[ngram][sentence[i + n]] += 1
                        else:
                            right_neighbors[ngram]["<EOS>"] += 1

        # ç­›é€‰æ–°è¯
        for word, freq in ngram_freq.items():
            if freq < config.new_word_discovery.new_word_min_freq:
                continue

            # é‚»æ¥ç†µ
            left_ent: float = calculate_entropy(left_neighbors[word])
            right_ent: float = calculate_entropy(right_neighbors[word])
            min_ent: float = min(left_ent, right_ent)

            if min_ent < config.new_word_discovery.entropy_threshold:
                continue

            # PMIï¼ˆå†…éƒ¨å‡èšåº¦ï¼‰
            min_pmi: float = float("inf")
            for i in range(1, len(word)):
                left_freq: int = ngram_freq.get(word[:i], 0)
                right_freq: int = ngram_freq.get(word[i:], 0)

                if left_freq > 0 and right_freq > 0:
                    pmi: float = math.log2(
                        (freq * total_chars) / (left_freq * right_freq + 1e-10)
                    )
                    min_pmi = min(min_pmi, pmi)

            if min_pmi == float("inf"):
                min_pmi = 0

            if min_pmi < config.new_word_discovery.pmi_threshold:
                continue

            self.discovered_words.add(word)

        # æ·»åŠ åˆ° jieba è¯å…¸
        for word in self.discovered_words:
            jieba.add_word(word, freq=1000)

        logger.info(f"   å‘ç° {len(self.discovered_words)} ä¸ªæ–°è¯")

    def _merge_word_pairs(self) -> None:
        """è¯ç»„åˆå¹¶"""
        bigram_counter: Counter[tuple[str, str]] = Counter()
        word_right_counter: Counter[str] = Counter()

        for text in self.cleaned_texts:
            words: list[str] = [w for w in jieba.cut(text) if w.strip()]

            for i in range(len(words) - 1):
                w1: str = words[i].strip()
                w2: str = words[i + 1].strip()

                if not w1 or not w2:
                    continue

                if re.match(r"^[\d\W]+$", w1) or re.match(r"^[\d\W]+$", w2):
                    continue

                bigram_counter[(w1, w2)] += 1
                word_right_counter[w1] += 1

        # æ‰¾å‡ºåº”è¯¥åˆå¹¶çš„è¯å¯¹
        for (w1, w2), count in bigram_counter.items():
            merged: str = w1 + w2

            if len(merged) > config.word_merge.merge_max_len:
                continue
            if count < config.word_merge.merge_min_freq:
                continue

            # æ¡ä»¶æ¦‚ç‡ P(w2|w1)
            if word_right_counter[w1] > 0:
                prob: float = count / word_right_counter[w1]
                if prob >= config.word_merge.merge_min_prob:
                    self.merged_words[merged] = (w1, w2, count, prob)
                    jieba.add_word(merged, freq=count * 1000)

        logger.info(f"   åˆå¹¶ {len(self.merged_words)} ä¸ªè¯ç»„")

        # æ˜¾ç¤ºå‰å‡ ä¸ª
        if self.merged_words:
            sorted_merges: list[tuple[str, tuple[str, str, int, float]]] = sorted(
                self.merged_words.items(), key=lambda x: -x[1][2]
            )[:10]
            for merged, (w1, w2, cnt, prob) in sorted_merges:
                logger.info(f"      {merged}: {w1}+{w2} ({cnt}æ¬¡, {prob:.0%})")

    def _tokenize_and_count(self) -> None:
        """åˆ†è¯ç»Ÿè®¡"""
        for msg in self.messages:
            if self._is_bot_message(msg):
                continue

            sender_uin: str | int = msg.sender.uin
            text: str = msg.content.text
            cleaned: str = clean_text(text)

            if not cleaned:
                continue

            words: list[str] = list(jieba.cut(cleaned))
            emojis: list[str] = extract_emojis(cleaned)
            words = [w for w in words if not is_emoji(w)]
            all_tokens: list[str] = words + emojis

            for word in all_tokens:
                word = word.strip()
                if not word:
                    continue

                # è·³è¿‡çº¯æ•°å­—/ç¬¦å·
                if re.match(r"^[\d\W]+$", word) and not is_emoji(word):
                    continue

                self.word_freq[word] += 1
                self.word_contributors[word][sender_uin] += 1

                if len(self.word_samples[word]) < config.analysis.sample_count * 3:
                    self.word_samples[word].append(cleaned)

    def _fun_statistics(self) -> None:
        """è¶£å‘³ç»Ÿè®¡"""
        prev_clean: str | None = None
        prev_sender: str | int | None = None

        for msg in self.messages:
            if self._is_bot_message(msg):
                continue

            sender_uin: str | int = msg.sender.uin
            text: str = msg.content.text
            timestamp: str = msg.timestamp

            self.user_msg_count[sender_uin] += 1
            clean: str = clean_text(text)
            self.user_char_count[sender_uin] += len(clean)

            # å›¾ç‰‡æ£€æµ‹ï¼ˆæ’é™¤ GIFï¼‰
            if "[å›¾ç‰‡:" in text and ".gif" not in text.lower():
                self.user_image_count[sender_uin] += 1

            # è½¬å‘æ£€æµ‹
            if "[åˆå¹¶è½¬å‘:" in text:
                self.user_forward_count[sender_uin] += 1

            # å›å¤ç»Ÿè®¡
            if msg.content.reply:
                self.user_reply_count[sender_uin] += 1
                ref_msg_id: str = msg.content.reply.referencedMessageId
                if ref_msg_id in self.msgid_to_sender:
                    target_uin: str | int = self.msgid_to_sender[ref_msg_id]
                    self.user_replied_count[target_uin] += 1

            # @ ç»Ÿè®¡
            for elem in msg.rawMessage.elements:
                if elem.elementType == 1 and elem.textElement:
                    at_type: int = elem.textElement.atType
                    at_uid: str = elem.textElement.atUid
                    if at_type > 0 and at_uid and at_uid != "0":
                        self.user_at_count[sender_uin] += 1
                        self.user_ated_count[at_uid] += 1

            # è¡¨æƒ…ç»Ÿè®¡ï¼ˆåŒ…æ‹¬ emojiã€[è¡¨æƒ…:]ã€GIFï¼‰
            emojis: list[str] = extract_emojis(clean)
            gif_count: int = text.lower().count(".gif")
            bracket_emoji_count: int = text.count("[è¡¨æƒ…:")
            emoji_count: int = len(emojis) + bracket_emoji_count + gif_count

            if emoji_count > 0:
                self.user_emoji_count[sender_uin] += emoji_count

            # é“¾æ¥ç»Ÿè®¡
            if "[é“¾æ¥:" in text or re.search(r"https?://", text):
                self.user_link_count[sender_uin] += 1

            # æ—¶æ®µç»Ÿè®¡
            hour: int | None = parse_timestamp(timestamp)
            if hour is not None:
                self.hour_distribution[hour] += 1
                if hour in config.time.night_owl_hours:
                    self.user_night_count[sender_uin] += 1
                if hour in config.time.early_bird_hours:
                    self.user_morning_count[sender_uin] += 1

            # å¤è¯»ç»Ÿè®¡ï¼ˆç”¨æ¸…ç†åæ–‡æœ¬ï¼Œä¸”å†…å®¹è¦æœ‰æ„ä¹‰ï¼‰
            if (
                clean
                and len(clean) >= 2
                and clean == prev_clean
                and sender_uin != prev_sender
            ):
                self.user_repeat_count[sender_uin] += 1

            prev_clean = clean if clean else prev_clean
            prev_sender = sender_uin

        # è®¡ç®—äººå‡å­—æ•°
        for uin in self.user_msg_count:
            msg_count: int = self.user_msg_count[uin]
            char_count: int = self.user_char_count[uin]
            if msg_count >= 10:
                self.user_char_per_msg[uin] = char_count / msg_count

    def _filter_results(self) -> None:
        """è¿‡æ»¤ç»“æœ"""
        filtered_freq: Counter[str] = Counter()

        for word, freq in self.word_freq.items():
            # é•¿åº¦è¿‡æ»¤
            if (
                len(word) < config.analysis.min_word_len
                or len(word) > config.analysis.max_word_len
            ):
                continue
            if freq < config.analysis.min_freq:
                continue

            # ç™½åå•ç›´æ¥é€šè¿‡
            if word in config.filter.whitelist:
                filtered_freq[word] = freq
                continue

            # é»‘åå•è·³è¿‡
            if word in config.filter.blacklist:
                continue

            # åœç”¨è¯ï¼ˆemoji é™¤å¤–ï¼‰
            if word in config.filter.stopwords and not is_emoji(word):
                continue

            # å•å­—ç‰¹æ®Šå¤„ç†
            if len(word) == 1:
                if is_emoji(word):
                    pass  # emoji ä¿ç•™
                else:
                    stats = self.single_char_stats.get(word)
                    if stats:
                        _, indep, ratio = stats
                        if (
                            ratio < config.single_char.single_min_solo_ratio
                            or indep < config.single_char.single_min_solo_count
                        ):
                            continue
                    else:
                        continue

            # çº¯æ•°å­—è·³è¿‡
            if re.match(r"^[\d\s]+$", word):
                continue

            # çº¯æ ‡ç‚¹è·³è¿‡
            if all(c in PUNCTUATION for c in word):
                continue

            filtered_freq[word] = freq

        self.word_freq = filtered_freq

        # é‡‡æ ·
        for word in self.word_samples:
            samples: list[str] = self.word_samples[word]
            if len(samples) > config.analysis.sample_count:
                self.word_samples[word] = random.sample(
                    samples, config.analysis.sample_count
                )

        logger.info(f"   è¿‡æ»¤å {len(self.word_freq)} ä¸ªè¯")

    def get_top_words(self, n: int | None = None) -> list[tuple[str, int]]:
        """è·å–é«˜é¢‘è¯

        Args:
            n: è¿”å›çš„è¯æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼

        Returns:
            (è¯, é¢‘ç‡) çš„åˆ—è¡¨
        """
        n = n or config.analysis.top_n
        return self.word_freq.most_common(n)

    def get_word_detail(self, word: str) -> dict[str, Any]:
        """è·å–è¯çš„è¯¦ç»†ä¿¡æ¯

        Args:
            word: è¯è¯­

        Returns:
            åŒ…å«è¯é¢‘ã€æ ·ä¾‹ã€è´¡çŒ®è€…çš„å­—å…¸
        """
        return {
            "word": word,
            "freq": self.word_freq.get(word, 0),
            "samples": self.word_samples.get(word, []),
            "contributors": [
                (self.get_name(uin), count)
                for uin, count in self.word_contributors[word].most_common(
                    config.analysis.contributor_top_n
                )
            ],
        }

    def get_fun_rankings(self) -> dict[str, list[tuple[str, Any]]]:
        """è·å–è¶£å‘³æ’è¡Œæ¦œ

        Returns:
            å„ç§æ’è¡Œæ¦œçš„å­—å…¸
        """
        rankings: dict[str, list[tuple[str, Any]]] = {}

        def fmt(
            counter: Counter[str | int], top_n: int = config.analysis.contributor_top_n
        ) -> list[tuple[str, Any]]:
            return [
                (self.get_name(uin), count) for uin, count in counter.most_common(top_n)
            ]

        rankings["è¯ç—¨æ¦œ"] = fmt(self.user_msg_count)
        rankings["å­—æ•°æ¦œ"] = fmt(self.user_char_count)

        sorted_avg: list[tuple[str | int, float]] = sorted(
            self.user_char_per_msg.items(), key=lambda x: x[1], reverse=True
        )[: config.analysis.contributor_top_n]
        rankings["é•¿æ–‡ç‹"] = [
            (self.get_name(uin), f"{avg:.1f}å­—/æ¡") for uin, avg in sorted_avg
        ]

        rankings["å›¾ç‰‡ç‹‚é­”"] = fmt(self.user_image_count)
        rankings["åˆå¹¶è½¬å‘ç‹"] = fmt(self.user_forward_count)
        rankings["å›å¤ç‹‚"] = fmt(self.user_reply_count)
        rankings["è¢«å›å¤æœ€å¤š"] = fmt(self.user_replied_count)
        rankings["è‰¾ç‰¹ç‹‚"] = fmt(self.user_at_count)
        rankings["è¢«è‰¾ç‰¹æœ€å¤š"] = fmt(self.user_ated_count)
        rankings["è¡¨æƒ…å¸"] = fmt(self.user_emoji_count)
        rankings["é“¾æ¥åˆ†äº«ç‹"] = fmt(self.user_link_count)
        rankings["æ·±å¤œå…š"] = fmt(self.user_night_count)
        rankings["æ—©èµ·é¸Ÿ"] = fmt(self.user_morning_count)
        rankings["å¤è¯»æœº"] = fmt(self.user_repeat_count)

        return rankings

    def export_json(self) -> dict[str, Any]:
        """å¯¼å‡º JSON æ ¼å¼ç»“æœï¼ˆåŒ…å« UIN ä¿¡æ¯ï¼‰

        Returns:
            å®Œæ•´çš„åˆ†æç»“æœå­—å…¸
        """
        result: dict[str, Any] = {
            "chatName": self.chat_name,
            "messageCount": len(self.messages),
            "topWords": [
                {
                    "word": word,
                    "freq": freq,
                    "contributors": [
                        {
                            "name": self.get_name(uin),
                            "uin": str(uin),
                            "count": count,
                        }
                        for uin, count in self.word_contributors[word].most_common(
                            config.analysis.contributor_top_n
                        )
                    ],
                    "samples": self.word_samples.get(word, [])[
                        : config.analysis.sample_count
                    ],
                }
                for word, freq in self.get_top_words()
            ],
            "rankings": {},
            "hourDistribution": {
                str(h): self.hour_distribution.get(h, 0) for h in range(24)
            },
        }

        # è¶£å‘³æ¦œå•ï¼ˆåŒ…å« UINï¼‰
        def fmt_with_uin(
            counter: Counter[str | int], top_n: int = config.analysis.contributor_top_n
        ) -> list[dict[str, Any]]:
            return [
                {"name": self.get_name(uin), "uin": str(uin), "value": count}
                for uin, count in counter.most_common(top_n)
            ]

        result["rankings"]["è¯ç—¨æ¦œ"] = fmt_with_uin(self.user_msg_count)
        result["rankings"]["å­—æ•°æ¦œ"] = fmt_with_uin(self.user_char_count)

        # é•¿æ–‡ç‹ç‰¹æ®Šå¤„ç†
        sorted_avg_export: list[tuple[str | int, float]] = sorted(
            self.user_char_per_msg.items(), key=lambda x: x[1], reverse=True
        )[: config.analysis.contributor_top_n]
        result["rankings"]["é•¿æ–‡ç‹"] = [
            {"name": self.get_name(uin), "uin": str(uin), "value": f"{avg:.1f}å­—/æ¡"}
            for uin, avg in sorted_avg_export
        ]

        result["rankings"]["å›¾ç‰‡ç‹‚é­”"] = fmt_with_uin(self.user_image_count)
        result["rankings"]["åˆå¹¶è½¬å‘ç‹"] = fmt_with_uin(self.user_forward_count)
        result["rankings"]["å›å¤ç‹‚"] = fmt_with_uin(self.user_reply_count)
        result["rankings"]["è¢«å›å¤æœ€å¤š"] = fmt_with_uin(self.user_replied_count)
        result["rankings"]["è‰¾ç‰¹ç‹‚"] = fmt_with_uin(self.user_at_count)
        result["rankings"]["è¢«è‰¾ç‰¹æœ€å¤š"] = fmt_with_uin(self.user_ated_count)
        result["rankings"]["è¡¨æƒ…å¸"] = fmt_with_uin(self.user_emoji_count)
        result["rankings"]["é“¾æ¥åˆ†äº«ç‹"] = fmt_with_uin(self.user_link_count)
        result["rankings"]["æ·±å¤œå…š"] = fmt_with_uin(self.user_night_count)
        result["rankings"]["æ—©èµ·é¸Ÿ"] = fmt_with_uin(self.user_morning_count)
        result["rankings"]["å¤è¯»æœº"] = fmt_with_uin(self.user_repeat_count)

        return result
